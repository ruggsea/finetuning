{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690381e1-d285-4eb6-9f57-5d3b3708f97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How's it going? Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "def generate(prompt:str):\n",
    "    response=client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\":prompt}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=8192\n",
    "    ).choices[0].message.content\n",
    "    return response\n",
    "\n",
    "\n",
    "generate(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a863a07d-b62b-44c9-8a85-273e8fba911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df=pd.read_parquet(\"wsdm/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78aeb49b-517f-47c9-ac00-aabed9cdad90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41370 cached entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48439/48439 [31:15<00:00, 25.83it/s]  \n"
     ]
    }
   ],
   "source": [
    "import json, os, aiohttp, asyncio, re\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "results_buffer = defaultdict(dict)\n",
    "buffer_size = 500\n",
    "\n",
    "def clean_json_string(s):\n",
    "    json_match = re.search(r'\\{.*\\}', s, re.DOTALL)\n",
    "    if json_match:\n",
    "        s = json_match.group(0)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            s = re.sub(r':\\s*\"([^\"]*)\"', lambda m: ': \"' + m.group(1).replace('\\n', ' ').replace('  ', ' ') + '\"', s)\n",
    "            s = s.replace('\\n', ' ').replace('\\\\', '\\\\\\\\').strip()\n",
    "            return json.loads(s)\n",
    "        except:\n",
    "            rationale = re.search(r'\"rationale\":\\s*\"([^\"]*)\"', s)\n",
    "            if rationale:\n",
    "                return {\"rationale\": rationale.group(1)}\n",
    "            return {\"rationale\": \"Failed to parse response\"}\n",
    "\n",
    "async def generate_response(session, semaphore, row):\n",
    "    async with semaphore:\n",
    "        url, headers = \"http://localhost:8000/v1/chat/completions\", {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer EMPTY\"}\n",
    "        winner = 'A' if row['winner'] == 'model_a' else 'B'\n",
    "        formatted_prompt = f\"\"\"Given these two responses to the prompt: \"{row['prompt']}\"\n",
    "Response A:\\n{row['response_a']}\\nResponse B:\\n{row['response_b']}\\nThe better response was: Response {winner}\n",
    "Explain step by step why Response {winner} was better than Response {'A' if winner == 'B' else 'B'}.\n",
    "IMPORTANT: Your response must be in valid JSON format with a single field called \"rationale\".\"\"\"\n",
    "\n",
    "        data = {\"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": formatted_prompt}], \n",
    "                \"temperature\": 0.7, \"max_tokens\": 2048}\n",
    "        \n",
    "        try:\n",
    "            async with session.post(url, headers=headers, json=data) as response:\n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "                    return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                print(f\"Error response code {response.status} for ID {row['id']}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Request error for ID {row['id']}: {str(e)}\")\n",
    "            await asyncio.sleep(1)\n",
    "            return None\n",
    "\n",
    "async def process_task(i, row, session, semaphore, pbar):\n",
    "    try:\n",
    "        response_content = await generate_response(session, semaphore, row)\n",
    "        if response_content:\n",
    "            parsed_response = clean_json_string(response_content)\n",
    "            row_dict = row.to_dict()\n",
    "            row_dict['rationale'] = str(parsed_response.get('rationale', 'Failed to extract rationale'))\n",
    "            results_buffer[row['id']] = row_dict\n",
    "            \n",
    "            if len(results_buffer) >= buffer_size:\n",
    "                await save_results('wsdm/semisynthetic_train.parquet')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i} (ID {row['id']}): {str(e)}\")\n",
    "        row_dict = row.to_dict()\n",
    "        row_dict['rationale'] = f\"Error: {str(e)}\"\n",
    "        results_buffer[row['id']] = row_dict\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "async def save_results(cache_path):\n",
    "    if not results_buffer:\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        if os.path.exists(cache_path):\n",
    "            df = pd.read_parquet(cache_path)\n",
    "            df = df[~df['id'].isin(results_buffer.keys())]\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "        \n",
    "        new_df = pd.DataFrame.from_records(list(results_buffer.values()))\n",
    "        result_df = pd.concat([df, new_df], ignore_index=True) if not df.empty else new_df\n",
    "        \n",
    "        # Convert rationale column to string type\n",
    "        result_df['rationale'] = result_df['rationale'].astype(str)\n",
    "        \n",
    "        result_df.to_parquet(cache_path)\n",
    "        results_buffer.clear()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "\n",
    "async def main(train_df):\n",
    "    os.makedirs('wsdm', exist_ok=True)\n",
    "    cache_path = 'wsdm/semisynthetic_train.parquet'\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        cached_df = pd.read_parquet(cache_path)\n",
    "        processed_ids = set(cached_df['id'].tolist())\n",
    "        processed_count = len(processed_ids)\n",
    "        print(f\"Loaded {processed_count} cached entries\")\n",
    "    else:\n",
    "        processed_ids = set()\n",
    "        processed_count = 0\n",
    "\n",
    "    semaphore = asyncio.Semaphore(500)\n",
    "    tasks = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        pbar = tqdm_asyncio(total=len(train_df))\n",
    "        pbar.update(processed_count)\n",
    "            \n",
    "        for i in range(len(train_df)):\n",
    "            row = train_df.iloc[i]\n",
    "            if row['id'] not in processed_ids:\n",
    "                tasks.append(process_task(i, row, session, semaphore, pbar))\n",
    "        \n",
    "        await asyncio.gather(*tasks)\n",
    "        \n",
    "        if results_buffer:\n",
    "            await save_results(cache_path)\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "    return pd.read_parquet(cache_path)\n",
    "\n",
    "result_df = await main(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9175aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "semisynthetic_train_df=pd.read_parquet(\"wsdm/semisynthetic_train.parquet\")\n",
    "\n",
    "# filter rows where rationale contains \"Error\" or \"Failed\"\n",
    "semisynthetic_train_df=semisynthetic_train_df[~semisynthetic_train_df['rationale'].str.contains(\"Error|Failed\")]\n",
    "semisynthetic_train_df.to_parquet(\"wsdm/semisynthetic_train.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
